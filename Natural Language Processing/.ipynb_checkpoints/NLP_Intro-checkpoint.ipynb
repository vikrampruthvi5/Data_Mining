{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text = 'In computer science, artificial intelligence (AI), sometimes called machine intelligence, \\\n",
    "is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans. \\\n",
    "Colloquially, the term artificial intelligence is often used to describe machines (or computers) that mimic cognitive functions \\\n",
    "that humans associate with the human mind, such as learning and problem solving \\\n",
    "\\\n",
    "    A search can be designed to return every match on a line, if there are more than\\\n",
    "one, or just the first match. In the following examples we generally underline the\\\n",
    "exact part of the pattern that matches the regular expression and show only the first\\\n",
    "match. Weâ€™ll show regular expressions delimited by slashes but note that slashes are\\\n",
    "not part of the regular expressions.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization : List of words and characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in given sentence : 133\n",
      "First 5 tokens:\n",
      "[\n",
      "   In\n",
      "   computer\n",
      "   science\n",
      "   ,\n",
      "   artificial\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.tokenize.word_tokenize(Text)\n",
    "print(\"Total tokens in given sentence : \" + str(len(tokens)))\n",
    "print(\"First 5 tokens:\\n[\")\n",
    "for i in tokens[:5]:\n",
    "    print(\"   \" + i)\n",
    "print(\"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words : Word frequency calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stemmed_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3cc8308ab05f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstemmed_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mfdist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfdist_top10\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfdist_top10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stemmed_words' is not defined"
     ]
    }
   ],
   "source": [
    "for i in stemmed_words:\n",
    "    fdist[i.lower()] += 1\n",
    "fdist\n",
    "fdist_top10 = fdist.most_common(10)\n",
    "fdist_top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi / Tri / NGrams\n",
    "Collection of consequent words.\n",
    "* bi-grams  : 2 consequent words\n",
    "* tri-grams : 3 consequent words\n",
    "* n-grams   : n consequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams, trigrams, ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigrams\n",
    "bi_grams = list(bigrams(tokens))\n",
    "tri_grams = list(trigrams(tokens))\n",
    "n_grams = list(ngrams(tokens, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiGrams\n",
      "('In', 'computer')\n",
      "('computer', 'science')\n",
      "('science', ',')\n",
      "(',', 'artificial')\n",
      "('artificial', 'intelligence')\n",
      "\n",
      "TriGrams\n",
      "('In', 'computer', 'science')\n",
      "('computer', 'science', ',')\n",
      "('science', ',', 'artificial')\n",
      "(',', 'artificial', 'intelligence')\n",
      "('artificial', 'intelligence', '(')\n",
      "\n",
      "NGrams\n",
      "('In', 'computer', 'science', ',')\n",
      "('computer', 'science', ',', 'artificial')\n",
      "('science', ',', 'artificial', 'intelligence')\n",
      "(',', 'artificial', 'intelligence', '(')\n",
      "('artificial', 'intelligence', '(', 'AI')\n"
     ]
    }
   ],
   "source": [
    "# bigrams\n",
    "print(\"BiGrams\")\n",
    "for i in bi_grams[:5]:\n",
    "    print(i)\n",
    "\n",
    "print(\"\\nTriGrams\")\n",
    "# trigrams\n",
    "for i in tri_grams[:5]:\n",
    "    print(i)\n",
    "\n",
    "print(\"\\nNGrams\")\n",
    "# ngrams\n",
    "for i in n_grams[:5]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming : getting root word or base form of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, ISRIStemmer, LancasterStemmer, RegexpStemmer, RSLPStemmer, SnowballStemmer\n",
    "pst = PorterStemmer()\n",
    "lst = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In\n",
      "comput\n",
      "scienc\n",
      ",\n",
      "artifici\n"
     ]
    }
   ],
   "source": [
    "stemmed_words = []\n",
    "for i in tokens:\n",
    "    stemmed_words.append(pst.stem(i))\n",
    "\n",
    "for i in stemmed_words[:5]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In\n",
      "comput\n",
      "scienc\n",
      ",\n",
      "artifici\n"
     ]
    }
   ],
   "source": [
    "lan_stemmed_words = []\n",
    "for i in tokens:\n",
    "    lan_stemmed_words.append(i.lower())\n",
    "\n",
    "for i in stemmed_words[:5]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization : Alternative to Stemming.\n",
    "* More powerful than just stemming.\n",
    "* Unlike stemming, output of lemmatizer is a proper word (stemming provides root form, not the word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import wordnet\n",
    "from nltk import WordNetLemmatizer\n",
    "word_lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'giving'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lem.lemmatize('giving')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lemmatizer has just returned the same word as POST tags are not assigned\n",
    "* If POS tags are not assigned, it assumes that all words are nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tags & Stop Words\n",
    "* NLTK by default comes with stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "me\n",
      "my\n",
      "myself\n",
      "we\n"
     ]
    }
   ],
   "source": [
    "for i in stopwords.words(\"english\")[:5]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fdist_top10' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d342304f4fbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfdist_top10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'fdist_top10' is not defined"
     ]
    }
   ],
   "source": [
    "fdist_top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Most of the words are either punctuations or the stop words\n",
    "* Lets remove punctuation first\n",
    "* Then stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "punctuation = re.compile(r'[-.,?!:;()|0-9]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_punctuation = []\n",
    "for words in tokens:\n",
    "    word = punctuation.sub(\"\", words)\n",
    "    if len(word) > 0:\n",
    "        post_punctuation.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In', 'computer', 'science', 'artificial', 'intelligence']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_5 = [x for x in post_punctuation[:5]]\n",
    "words_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging\n",
    "* input : Tokens\n",
    "* Output: word, POS Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT')]\n",
      "[('dog', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('in', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('water', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "sent1 = \"The dog is in the water\"\n",
    "sen1_tokens = nltk.tokenize.word_tokenize(sent1)\n",
    "\n",
    "for i in sen1_tokens:\n",
    "    print(pos_tag([i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('John', 'NNP')]\n",
      "[('is', 'VBZ')]\n",
      "[('eating', 'VBG')]\n",
      "[('his', 'PRP$')]\n",
      "[('food', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "sent2 = \"John is eating his food\"\n",
    "sen2_tokens = nltk.tokenize.word_tokenize(sent2)\n",
    "\n",
    "for i in sen2_tokens:\n",
    "    print(pos_tag([i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'is' and 'eating' are together but considered as verbs. Which is wrong.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity recognition\n",
    "There are 3 major types of identification\n",
    "1. Non phrase identification : Deep parsing and pos tagging\n",
    "2. Phrase classification : Identifies nouns into categories (google -> organization, sundar -> person)\n",
    "3. Entity disambuiguation : Creates one more layer of validation\n",
    "    * IBM Watson\n",
    "    * Google graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('US', 'NNP'),\n",
       " ('President', 'NNP'),\n",
       " ('stays', 'VBZ'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('white', 'JJ'),\n",
       " ('house', 'NN')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'The US President stays in the white house'\n",
    "sen_tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "sen_pos = nltk.pos_tag(sen_tokens)\n",
    "sen_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_ner = nltk.ne_chunk(sen_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  (ORGANIZATION US/NNP)\n",
      "  President/NNP\n",
      "  stays/VBZ\n",
      "  in/IN\n",
      "  the/DT\n",
      "  white/JJ\n",
      "  house/NN)\n"
     ]
    }
   ],
   "source": [
    "print(sen_ner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYNTAX : Process, rules and principles that govern the structure of a sentence\n",
    "* Syntactical structure of a sentence\n",
    "* Tree like structure\n",
    "* Defines which part of sentence comes after which part\n",
    "\n",
    "## Ghost script\n",
    "* ghost script is needed to visualize the tree\n",
    "* download : https://www.ghostscript.com/download/gsdnld.html\n",
    "* Install  : https://www.ghostscript.com/doc/9.27/Install.htm\n",
    "    1. gunzip <file.tar.gz>\n",
    "    2. tar -xvf <file.tar>\n",
    "    3. cd <folder>\n",
    "    4. ./configure\n",
    "    5. make install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
